{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic analysis\n",
    "\n",
    "NOTE: This requires a python virtualenv already installed with the appropriate dependencies, by e.g.:\n",
    "\n",
    "`conda create -n acanaba python=3.10 numpy pandas ipykernel jupyter holoviews bokeh datashader seaborn pytables`\n",
    "\n",
    "Let's load our tracked data and calculate some basic metrics for the pre-reversal and first reversal session.\n",
    "\n",
    "First we need to import the `pandas` module which allows us to manipulate tabular data. The module provides two basic data structures, DataFrames (table-like) and Series (row-like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load our analysed position data. Initially let's just load up the first data file and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.read_hdf('../data/operant2022/sub-lizzy/sub-lizzy_ses-prerev_task-RR10_vidDLC_resnet50_dlc_acan_masterOct23shuffle1_800000.h5')\n",
    "_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is reasonable, but there seems to be an uninformative (and incorrect) 'scorer' index, and the frame number has no name. Let's fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.columns = _df.columns.droplevel(0)\n",
    "_df.index.name = 'frame_id'\n",
    "_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, this looks better. Now let's load all the sessions we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import re\n",
    "Recording = namedtuple(\n",
    "    \"Recording\", [\"subject\", \"session\", \"task\", \"acq\", \"med_path\", \"dlc_path\"])\n",
    "pattern = (f'sub-*/ses-*/sub-*_ses-*_task-*_acq-*_med.txt')\n",
    "regex_pattern = (r\"sub-([^_]+)_ses-([^_]+)_task-([^_]+)_acq-([^_]+)_med.txt\")\n",
    "data_files = list(Path('../data/operant2022/').glob(str(pattern)))\n",
    "extracted_data = []\n",
    "for file_path in data_files:\n",
    "    match = re.search(regex_pattern, str(file_path.name))\n",
    "    if match:\n",
    "        sub, ses, task, acq = match.groups()\n",
    "        dlc_path = re.sub(\n",
    "            r'acq-.*_med\\.txt',\n",
    "            'vidDLC_resnet50_dlc_acan_masterOct23shuffle1_800000.h5',\n",
    "            str(file_path))\n",
    "        data_file = Recording(sub, ses, task, acq, file_path, dlc_path)\n",
    "        extracted_data.append(data_file)\n",
    "recordings = pd.DataFrame(extracted_data)\n",
    "recordings = recordings.set_index(['subject', 'session', 'task', 'acq'])\n",
    "recordings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function to load a tracking session using the filename (embedded in one row of the above table), so we can load all our data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_track_session(session: pd.Series) -> pd.DataFrame:\n",
    "    '''Load one session analysed by DeepLabCut into a DataFrame'''\n",
    "    try:\n",
    "        df = pd.read_hdf(session.iloc[0])\n",
    "    except FileNotFoundError:\n",
    "        print(f'File not found: {session.iloc[0]}')\n",
    "        return None\n",
    "    df.columns = df.columns.droplevel(0)\n",
    "    df.index.name = 'frame_id'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load all the tracking data in one big DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df = recordings.dlc_path.groupby(['subject', 'session', 'task', 'acq']).apply(load_track_session)\n",
    "track_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to extract one point, let's say the animal's nose, we can index just those columns.\n",
    "\n",
    "The `pd.IndexSlice` class allows us to do complex indexing on our DataFrames and Series objects - for example to extract only the coordinates of the nose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "nose_df = track_df.loc[:, idx['nose', :]]\n",
    "nose_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if we want to see just the frames where DeepLabCut classified a point with high confidence, we can filter the results on the likelihood.\n",
    "\n",
    "NOTE: We're using a boolean indexer here (a `pd.Series` of True/False values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nose_df.loc[nose_df.loc[:, idx['nose', 'likelihood']] > 0.95, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the session timings.\n",
    "\n",
    "Each day we ran two MedPC sessions, one for each lever, however we only recorded a single video that spanned both sessions. We somehow need to map the times of the MedPC data to the correct video frames. For this purpose, we had MedPC control an LED positioned in the frame of each video and record the onset and offset times for every LED flash.\n",
    "\n",
    "We can filter all our positional data by the first and last LED flash in each acquisition run. These values are stored in JSON files associated with each MedPC session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sidecar(filename: str) -> pd.Series:\n",
    "    with open(filename) as sidecar:\n",
    "        info = json.load(sidecar)\n",
    "    return pd.Series(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidecar_fns = Path('../data/operant2022').glob('*/*.json')\n",
    "info_df = pd.DataFrame([load_sidecar(fn) for fn in sidecar_fns]).set_index(['sub', 'ses', 'acq'])\n",
    "info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can label the tracking data with the session and acquisition information.\n",
    "\n",
    "First let's look at how we can select the correct data for each acquisition session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df.loc[idx['rev01', 22647:43448], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acq_session(info):\n",
    "    df = track_df.loc[idx[info.ses, info.firston:info.laston], :]\n",
    "    df = df.assign(acq=info.acq).set_index('acq', append=True)\n",
    "    df.index = df.index.reorder_levels(['session_id', 'acq', 'frame_id'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_df = pd.concat([get_acq_session(info) for info in info_df.reset_index().itertuples()])\n",
    "acq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "import datashader as ds\n",
    "from holoviews.operation.datashader import datashade\n",
    "hv.extension('bokeh')\n",
    "import panel\n",
    "panel.extension(comms='vscode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nose_df = acq_df.loc[idx['rev01', 'LP', :], idx['nose']].query('likelihood > 0.95')\n",
    "datashade(hv.Path(nose_df.loc[nose_df['likelihood'] > 0.95])).opts(width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {(ses, acq): hv.Path(acq_df.loc[idx[ses, acq, :], idx['nose']].query('likelihood > 0.95'))\n",
    "         for i, ses, acq in info_df.reset_index().loc[:, ['ses', 'acq']].itertuples()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.HoloMap(paths, kdims=['ses', 'acq']).layout().cols(2).opts(width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the centre point of the head? We need the mean point between the two ears, filtered for only those points where the likelihoods of both points are above a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_centre_mask = (acq_df.stack().loc[idx[:, :, :, 'likelihood'], idx['leftEar', 'rightEar']] > 0.95).all(axis=1).to_numpy()\n",
    "head_centre_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_centre_df = acq_df.stack().loc[idx[:, :, :, ['x', 'y']], ['leftEar', 'rightEar']].mean(axis=1).unstack()\n",
    "head_centre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {(ses, acq): hv.Path(head_centre_df.loc[idx[ses, acq, head_centre_mask], :])\n",
    "         for i, ses, acq in info_df.reset_index().loc[:, ['ses', 'acq']].itertuples()}\n",
    "hv.HoloMap(paths, kdims=['ses', 'acq']).layout().cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're plotting trajectories, but what about dwell times? Occupancy maps can give us dwell time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spatial\n",
    "bins = np.array((40, 40))\n",
    "minmax = [head_centre_df.loc[head_centre_mask, ['x', 'y']].min(),\n",
    "          head_centre_df.loc[head_centre_mask, ['x', 'y']].max()]\n",
    "path_range = [(a, b) for a, b in zip(*minmax)]\n",
    "head_centre_df['t'] = head_centre_df.index.get_level_values('frame_id') / 30\n",
    "_df = head_centre_df.loc[head_centre_mask, ['t', 'x', 'y']]\n",
    "occ = spatial.occupancy_map(_df.loc[idx['prerev', 'LG', :]].to_numpy(),\n",
    "                            bins=bins, smooth=1, max_dt=0.2,\n",
    "                            range=path_range)\n",
    "hv.Image(occ.hist.T).opts(cmap='viridis', frame_height=400,\n",
    "                          data_aspect=(bins[1]/bins[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "38c0db2b5ca17c1c4aac225b47f1dc3192e0458b2f2f724aaf9a19882846a9a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
